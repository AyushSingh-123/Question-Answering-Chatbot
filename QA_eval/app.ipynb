{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening and transcribing in real-time... Press Ctrl+C to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language 'en' with probability 0.493422\n",
      "[0.00s -> 0.04s]  Thank you.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Initialize the Whisper model\n",
    "model_size = \"large-v3\"\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float32\")\n",
    "\n",
    "# Parameters for real-time audio capture\n",
    "SAMPLE_RATE = 16000  # Whisper expects 16 kHz audio\n",
    "CHUNK_SIZE = 1024    # Size of each audio chunk to process\n",
    "DURATION = 5         # Duration of each recording chunk in seconds\n",
    "\n",
    "def real_time_transcription():\n",
    "    # Callback function to process audio chunks in real-time\n",
    "    def callback(indata, frames, time, status):\n",
    "        if status:\n",
    "            print(status)\n",
    "        audio_data = indata[:, 0]  \n",
    "        \n",
    "        # Resample and normalize audio if needed (Whisper expects float32 values)\n",
    "        audio_data = np.float32(audio_data)\n",
    "        \n",
    "        # Run transcription on the audio chunk\n",
    "        segments, info = model.transcribe(audio_data, beam_size=5)\n",
    "\n",
    "        # Output detected language and transcription\n",
    "        print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "        for segment in segments:\n",
    "            print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "\n",
    "    # Open an audio stream for real-time processing\n",
    "    with sd.InputStream(samplerate=SAMPLE_RATE, channels=1, callback=callback, blocksize=CHUNK_SIZE):\n",
    "        print(\"Listening and transcribing in real-time... Press Ctrl+C to stop.\")\n",
    "        sd.sleep(DURATION * 1000)  # Keep running for `DURATION` seconds (or set an infinite loop)\n",
    "\n",
    "# Start the real-time transcription\n",
    "real_time_transcription()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyaudio in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.14)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening and transcribing in real-time... Press Ctrl+C to stop.\n",
      "\n",
      "Detected language: 'en' with probability: 0.50\n",
      "Transcription:\n",
      "[0.00s -> 0.04s]  Thank you.\n",
      "input overflow\n",
      "\n",
      "Detected language: 'ru' with probability: 0.19\n",
      "Transcription:\n",
      "[0.00s -> 0.04s]  Субтитры создавал DimaTorzok\n",
      "input overflow\n",
      "\n",
      "Detected language: 'ru' with probability: 0.20\n",
      "Transcription:\n",
      "[0.00s -> 0.04s]  Субтитры сделал DimaTorzok\n",
      "input overflow\n",
      "Stopping transcription.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected language: 'ru' with probability: 0.19\n",
      "Transcription:\n",
      "[0.00s -> 0.04s]  Субтитры создавал DimaTorzok\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "# Initialize the Whisper model\n",
    "model_size = \"large-v3\"\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float32\")\n",
    "\n",
    "# Parameters for real-time audio capture\n",
    "SAMPLE_RATE = 16000  # Whisper expects 16 kHz audio\n",
    "CHUNK_SIZE = 1024 \n",
    "DURATION = 5   \n",
    "  # Size of each audio chunk to process\n",
    "\n",
    "# Create a queue to store transcriptions\n",
    "transcription_queue = queue.Queue()\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    \"\"\"Callback function to process incoming audio data.\"\"\"\n",
    "    if status:\n",
    "        print(status)\n",
    "    audio_data = indata[:, 0]  # Extract audio from the first channel\n",
    "    audio_data = np.float32(audio_data)  # Ensure audio data is float32\n",
    "\n",
    "    # Run transcription on the audio chunk\n",
    "    segments, info = model.transcribe(audio_data, beam_size=5)\n",
    "\n",
    "    # Output detected language and transcription\n",
    "    detected_language = info.language\n",
    "    language_probability = info.language_probability\n",
    "    transcription = []\n",
    "\n",
    "    for segment in segments:\n",
    "        transcription.append(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "\n",
    "    # Put the transcription in the queue\n",
    "    transcription_queue.put((detected_language, language_probability, transcription))\n",
    "\n",
    "def transcribe_audio():\n",
    "    \"\"\"Thread function to continuously print transcriptions.\"\"\"\n",
    "    while True:\n",
    "        detected_language, language_probability, transcription = transcription_queue.get()\n",
    "        \n",
    "        # Print the detected language and its probability\n",
    "        print(f\"\\nDetected language: '{detected_language}' with probability: {language_probability:.2f}\")\n",
    "        print(\"Transcription:\")\n",
    "        print(\"\\n\".join(transcription))\n",
    "\n",
    "def real_time_transcription():\n",
    "    \"\"\"Start the audio input stream and transcription thread.\"\"\"\n",
    "    # Start the audio stream\n",
    "    with sd.InputStream(samplerate=SAMPLE_RATE, channels=1, callback=audio_callback, blocksize=CHUNK_SIZE):\n",
    "        print(\"Listening and transcribing in real-time... Press Ctrl+C to stop.\")\n",
    "\n",
    "        # Start a separate thread for printing transcriptions\n",
    "        transcription_thread = threading.Thread(target=transcribe_audio, daemon=True)\n",
    "        transcription_thread.start()\n",
    "\n",
    "        # Keep the main thread alive to listen for keyboard interrupts\n",
    "        try:\n",
    "            while True:\n",
    "                sd.sleep(1000)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Stopping transcription.\")\n",
    "\n",
    "# Start the real-time transcription\n",
    "if __name__ == \"__main__\":\n",
    "    real_time_transcription()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening and transcribing in real-time... Press Ctrl+C to stop.\n",
      "\n",
      "Detected language: 'en' with probability: 0.51\n",
      "[0.00s -> 0.04s]  Thank you.\n",
      "input overflow\n",
      "Stopping transcription.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected language: 'ru' with probability: 0.21\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Initialize the Whisper model\n",
    "model_size = \"large-v3\"\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float32\")\n",
    "\n",
    "# Parameters for real-time audio capture\n",
    "SAMPLE_RATE = 16000  # Whisper expects 16 kHz audio\n",
    "CHUNK_SIZE = 1024    # Size of each audio chunk to process\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    \"\"\"Callback function to process incoming audio data.\"\"\"\n",
    "    if status:\n",
    "        print(status)\n",
    "    audio_data = indata[:, 0]  # Extract audio from the first channel\n",
    "    audio_data = np.float32(audio_data)  # Ensure audio data is float32\n",
    "\n",
    "    # Run transcription on the audio chunk\n",
    "    segments, info = model.transcribe(audio_data, beam_size=5)\n",
    "\n",
    "    # Output detected language and transcription immediately\n",
    "    detected_language = info.language\n",
    "    language_probability = info.language_probability\n",
    "\n",
    "    # Print detected language\n",
    "    print(f\"\\nDetected language: '{detected_language}' with probability: {language_probability:.2f}\")\n",
    "    \n",
    "    # Print transcription segments\n",
    "    for segment in segments:\n",
    "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "\n",
    "def real_time_transcription():\n",
    "    \"\"\"Start the audio input stream for real-time transcription.\"\"\"\n",
    "    # Start the audio stream\n",
    "    with sd.InputStream(samplerate=SAMPLE_RATE, channels=1, callback=audio_callback, blocksize=CHUNK_SIZE):\n",
    "        print(\"Listening and transcribing in real-time... Press Ctrl+C to stop.\")\n",
    "\n",
    "        # Keep the main thread alive to listen for keyboard interrupts\n",
    "        try:\n",
    "            while True:\n",
    "                sd.sleep(1000)  # Sleep in the main thread to keep it running\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Stopping transcription.\")\n",
    "\n",
    "# Start the real-time transcription\n",
    "if __name__ == \"__main__\":\n",
    "    real_time_transcription()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening and transcribing in real-time... Press Ctrl+C to stop.\n",
      "\n",
      "Detected language: 'en' with probability: 0.39\n",
      "[0.00s -> 0.04s]  Thank you.\n",
      "input overflow\n",
      "Stopping transcription.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected language: 'en' with probability: 0.27\n",
      "[0.00s -> 0.04s]  Thank you.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Initialize the Whisper model\n",
    "model_size = \"large-v3\"\n",
    "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"float32\")\n",
    "\n",
    "# Parameters for real-time audio capture\n",
    "SAMPLE_RATE = 16000  # Whisper expects 16 kHz audio\n",
    "CHUNK_SIZE = 512     # Reduced size of each audio chunk\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    \"\"\"Callback function to process incoming audio data.\"\"\"\n",
    "    if status:\n",
    "        print(status)\n",
    "    audio_data = indata[:, 0]  # Extract audio from the first channel\n",
    "    audio_data = np.float32(audio_data)  # Ensure audio data is float32\n",
    "\n",
    "    # Run transcription on the audio chunk\n",
    "    segments, info = model.transcribe(audio_data, beam_size=3)  # Lower beam size\n",
    "\n",
    "    # Output detected language and transcription immediately\n",
    "    detected_language = info.language\n",
    "    language_probability = info.language_probability\n",
    "\n",
    "    # Print detected language\n",
    "    print(f\"\\nDetected language: '{detected_language}' with probability: {language_probability:.2f}\")\n",
    "    \n",
    "    # Print transcription segments\n",
    "    for segment in segments:\n",
    "        print(f\"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}\")\n",
    "\n",
    "def real_time_transcription():\n",
    "    \"\"\"Start the audio input stream for real-time transcription.\"\"\"\n",
    "    # Start the audio stream\n",
    "    with sd.InputStream(samplerate=SAMPLE_RATE, channels=1, callback=audio_callback, blocksize=CHUNK_SIZE):\n",
    "        print(\"Listening and transcribing in real-time... Press Ctrl+C to stop.\")\n",
    "\n",
    "        # Keep the main thread alive to listen for keyboard interrupts\n",
    "        try:\n",
    "            while True:\n",
    "                sd.sleep(1000)  # Sleep in the main thread to keep it running\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Stopping transcription.\")\n",
    "\n",
    "# Start the real-time transcription\n",
    "if __name__ == \"__main__\":\n",
    "    real_time_transcription()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording for 5 seconds...\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wave\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Parameters for audio recording\n",
    "SAMPLE_RATE = 16000  # Whisper expects 16 kHz audio\n",
    "DURATION = 5         # Duration of recording in seconds\n",
    "CHUNK_SIZE = 1024    # Size of each audio chunk to process\n",
    "\n",
    "# Function to record audio\n",
    "def record_audio(duration, sample_rate):\n",
    "    \"\"\"Record audio from the microphone.\"\"\"\n",
    "    print(f\"Recording for {duration} seconds...\")\n",
    "    recorded_audio = []\n",
    "\n",
    "    # Callback function to capture audio\n",
    "    def callback(indata, frames, time, status):\n",
    "        if status:\n",
    "            print(status)\n",
    "        recorded_audio.append(indata.copy())\n",
    "\n",
    "    # Open the audio stream for recording\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=1, callback=callback, blocksize=CHUNK_SIZE):\n",
    "        sd.sleep(duration * 1000)  # Record for the specified duration\n",
    "\n",
    "    # Concatenate all recorded chunks into a single numpy array\n",
    "    return np.concatenate(recorded_audio)\n",
    "\n",
    "# Function to save audio to a WAV file\n",
    "def save_audio_to_wav(audio_data, filename, sample_rate):\n",
    "    \"\"\"Save recorded audio data to a WAV file.\"\"\"\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)          # Mono\n",
    "        wf.setsampwidth(2)          # 16-bit PCM\n",
    "        wf.setframerate(sample_rate) # Sample rate\n",
    "        wf.writeframes(audio_data.tobytes())  # Write audio data to file\n",
    "\n",
    "# Function to transcribe audio\n",
    "def transcribe_audio(filename):\n",
    "    \"\"\"Transcribe audio from a file using the Whisper model.\"\"\"\n",
    "    # Initialize the Whisper model\n",
    "    model_size = \"large-v3\"\n",
    "    model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float32\")\n",
    "\n",
    "    # Transcribe audio from the file\n",
    "    segments, info = model.transcribe(filename, beam_size=5)\n",
    "\n",
    "    # Output detected language and transcription\n",
    "    print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "    for segment in segments:\n",
    "        print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "\n",
    "# Main workflow\n",
    "if __name__ == \"__main__\":\n",
    "    # Record audio\n",
    "    audio_data = record_audio(DURATION, SAMPLE_RATE)\n",
    "\n",
    "    # Save the recorded audio to a WAV file\n",
    "    audio_filename = \"recorded_audio.wav\"\n",
    "    save_audio_to_wav(audio_data, audio_filename, SAMPLE_RATE)\n",
    "\n",
    "    # Transcribe the saved audio file\n",
    "    transcribe_audio(audio_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... (Press Ctrl+C to stop)\n",
      "Listening for speech...\n",
      "Customer:  Is it working? How are you? The body is working or not? Hello? Hello?\n",
      "Speaking pace: 1.40 words per second\n",
      "Listening for speech...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wave\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Default configurations\n",
    "DEFAULT_MODEL_SIZE = \"medium\"\n",
    "DEFAULT_CHUNK_LENGTH = 10  # seconds\n",
    "\n",
    "\n",
    "def calculate_speaking_pace(transcription, chunk_length):\n",
    "    \"\"\"Calculate the speaking pace in words per second.\"\"\"\n",
    "    words = transcription.split()\n",
    "    num_words = len(words)\n",
    "    speaking_rate = num_words / chunk_length  # Words per second\n",
    "    return speaking_rate\n",
    "\n",
    "\n",
    "def is_silence(data, max_amplitude_threshold=3000):\n",
    "    \"\"\"Check if audio data contains silence based on max amplitude.\"\"\"\n",
    "    max_amplitude = np.max(np.abs(data))\n",
    "    return max_amplitude <= max_amplitude_threshold\n",
    "\n",
    "\n",
    "def record_audio_chunk(audio, stream, chunk_length=DEFAULT_CHUNK_LENGTH):\n",
    "    \"\"\"Record a chunk of audio and save it to a temporary file.\"\"\"\n",
    "    frames = []\n",
    "    for _ in range(0, int(16000 / 1024 * chunk_length)):\n",
    "        data = stream.read(1024)\n",
    "        frames.append(data)\n",
    "\n",
    "    # Save the recorded chunk to a temporary WAV file\n",
    "    temp_file_path = 'temp_audio_chunk.wav'\n",
    "    with wave.open(temp_file_path, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(audio.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(16000)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "    # Check if the recorded chunk contains silence\n",
    "    try:\n",
    "        samplerate, data = wavfile.read(temp_file_path)\n",
    "        if is_silence(data):\n",
    "            os.remove(temp_file_path)\n",
    "            return False  # Return False if it's silence\n",
    "        return True  # Return True if it contains speech\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reading audio file: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def transcribe_audio(model, file_path):\n",
    "    \"\"\"Transcribe the audio file using the Whisper model.\"\"\"\n",
    "    segments, info = model.transcribe(file_path, beam_size=7)\n",
    "    transcription = ' '.join(segment.text for segment in segments)\n",
    "    return transcription\n",
    "\n",
    "\n",
    "def audio_to_text():\n",
    "    \"\"\"Main function to record and transcribe audio.\"\"\"\n",
    "    model_size = DEFAULT_MODEL_SIZE + \".en\"\n",
    "    model = WhisperModel(model_size, device=\"cpu\")\n",
    "\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=pyaudio.paInt16, channels=1, rate=16000,\n",
    "                        input=True, frames_per_buffer=1024)\n",
    "    \n",
    "    try:\n",
    "        print(\"Recording... (Press Ctrl+C to stop)\")\n",
    "        while True:\n",
    "            # Record audio chunk\n",
    "            print(\"Listening for speech...\")\n",
    "            if record_audio_chunk(audio, stream):\n",
    "                # Transcribe audio if speech is detected\n",
    "                transcription = transcribe_audio(model, 'temp_audio_chunk.wav')\n",
    "                os.remove('temp_audio_chunk.wav')\n",
    "\n",
    "                # Calculate and print speaking pace\n",
    "                speaking_pace = calculate_speaking_pace(transcription, DEFAULT_CHUNK_LENGTH)\n",
    "                print(f\"Customer: {transcription}\")\n",
    "                print(f\"Speaking pace: {speaking_pace:.2f} words per second\")\n",
    "            else:\n",
    "                print(\"Silence detected, waiting for speech...\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nStopping...\")\n",
    "\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        audio.terminate()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_to_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.2\n",
      "[notice] To update, run: c:\\Users\\ayush\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.1.24)\n",
      "Requirement already satisfied: certifi==2023.7.22 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (2023.7.22)\n",
      "Requirement already satisfied: chardet==4.0.0 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (4.0.0)\n",
      "Requirement already satisfied: cycler==0.10.0 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (0.10.0)\n",
      "Requirement already satisfied: idna==2.10 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (2.10)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (1.4.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (1.26.4)\n",
      "Requirement already satisfied: opencv-python-headless==4.8.0.74 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (4.8.0.74)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (10.4.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\ayush\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (2.8.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (1.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (2.31.0)\n",
      "Requirement already satisfied: six in c:\\users\\ayush\\appdata\\roaming\\python\\python311\\site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (1.26.19)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from roboflow) (0.4.27)\n",
      "Requirement already satisfied: colorama in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->roboflow) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->roboflow) (4.49.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->roboflow) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->roboflow) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ayush\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->roboflow) (3.3.2)\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in Food_detection-13 to multiclass: 100% [10738545 / 10738545] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to Food_detection-13 in multiclass:: 100%|██████████| 475/475 [00:02<00:00, 187.97it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"i26yzPgXSLDL6CMF0ZfF\")\n",
    "project = rf.workspace(\"project-eqxcg\").project(\"food_detection-vmjvx\")\n",
    "version = project.version(13)\n",
    "dataset = version.download(\"multiclass\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\ayush\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa530b7caea45c8a4e7c285dc5ca8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayush\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "# Setup your model and tokenizer (same as your script above)\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "config = PeftConfig.from_pretrained(\"D:\\project\")\n",
    "model = PeftModel.from_pretrained(base_model, \"D:\\project\")\n",
    "\n",
    "\n",
    "# Function to generate output based on input prompt\n",
    "def generate_output(prompt):\n",
    "    max_new_tokens = 200\n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**model_input, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded_output\n",
    "\n",
    "\n",
    "# Define Flask routes\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')  # Main page with the form\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    prompt = request.form['prompt']\n",
    "    generated_text = generate_output(prompt)  # Call generate_output from model\n",
    "    return jsonify({'generated_text': generated_text})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
